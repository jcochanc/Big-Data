---
title: "Homework 2"
author: "Jerson R. Cochancela"
date:  "`r format(Sys.time(), '%B %d, %Y')`"
output: pdf_document
---

```{r, echo = FALSE}
# Libraries.
pacman::p_load(dplyr, kableExtra, knitr, ggplot2, 
               ISLR, caret, scatterplot3d, rgl, calibrate, plotrix,
               pROC, e1071)
```

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.pos = 'H')
```

Book “An Introduction to Statistical Learning” by James G, Witten D, Hastie T, and Tibshirani (JWHT)

------------

## JWHT. Chp 9 Question 1.

### This problem involves hyperplanes in two dimensions.

### (a) Sketch the hyperplane $1 + 3X_1 - X_2 = 0$. Indicate the set of points for which $1 + 3X_1 - X2 > 0$, as well as the set of points for which $1 + 3X_1 - X_2 < 0$.

```{r, echo = FALSE, fig.align = "center", fig.asp = .78, fig.width=4.5, warning = FALSE}
# First we initialize x1 and x2
x1 <- x2 <- seq(-1.5, 1.5, length.out = 10)

# Place points on a grid to plot.
grid <- expand.grid("x1" = x1,
                    "x2" = x2 )

# Create a classifier based on the question.
classifier <- grid$x2 <= (1+3*grid$x1)

plot(grid$x1[classifier == TRUE], grid$x2[classifier == TRUE], col = "lightblue", xlim = c(-1.5, 1.5), ylim = c(-1.5, 1.5),
     xlab = expression(italic(X)[1]), ylab = expression(italic(X)[2]))
points(grid$x1[classifier == FALSE], grid$x2[classifier == FALSE], col = "lightpink")
abline(a = 1, b = 3, col ="black")
```



### (b) On the same plot, sketch the hyperplane $-2 + X_1 + 2X_2 = 0$. Indicate the set of points for which $-2+ X_1 +2X_2 > 0$, as well as the set of points for which $-2+ X_1 + 2X_2 < 0$.

```{r, echo = FALSE, fig.align = "center", fig.asp = .78, fig.width=4.5, warning = FALSE}
# Create a classifier based on the question.
classifier_2 <- grid$x2 <=  (1-0.5*grid$x1)


plot(grid$x1[classifier == TRUE & classifier_2 == TRUE ],
     grid$x2[classifier == TRUE & classifier_2 == TRUE ],
     col = "lightblue", xlim = c(-1.5, 1.5), ylim = c(-1.5, 1.5),
     xlab = expression(italic(X)[1]), ylab = expression(italic(X)[2]))
points(grid$x1[classifier == FALSE & classifier_2 == FALSE ],
       grid$x2[classifier == FALSE & classifier_2 == FALSE],
       col = "lightpink")
points(grid$x1[classifier == TRUE & classifier_2 == FALSE ],
       grid$x2[classifier == TRUE & classifier_2 == FALSE],
       col = "green")
points(grid$x1[classifier == FALSE & classifier_2 == TRUE ],
       grid$x2[classifier == FALSE & classifier_2 == TRUE],
       col = "purple")
abline(a = 1, b = 3, col ="black") # hyperplane 1
abline(a = 1, b = -0.5, col ="red") # hyperplane 2
```

## JWHT. Chp 9 Question 2.

### We have seen that in $p = 2$ dimensions, a linear decision boundary takes the form $\beta_0 + \beta_1 X_1 + \beta_2 X_2 = 0$.We now investigate a non-linear decision boundary.

### (a) Sketch the curve

$$(1 + X_1)^2 + (2 - X_2)^2 = 4.$$

### (b) On your sketch, indicate the set of points for which

$$ (1 + X_1)^2 + (2 - X_2)^2 > 4,$$

### as well as the set of points for which

$$ (1 + X_1)^2 + (2 - X_2)^2 \leq 4.$$

First note that in terms of $X_2$, we have the follow:

$$ (1 + X_1)^2 + (2 - X_2)^2 = 4 \Longrightarrow  (2 - X_2)^2 = 4 - (1 + X_1)^2$$

$$\therefore X_2 = 2 \pm \sqrt{4 - (1 + X_1)^2}$$

We will use this to draw the classifying curve.

### (c) Suppose that a classifier assigns an observation to the blue class if

$$(1 + X_1)^2 + (2 - X_2)^2 > 4,$$


### and to the red class otherwise. To what class is the observation $(0, 0)$ classified? $(-1, 1)$? $(2, 2)$? $(3, 8)$?

We fit (a), (b), and (c) on the following plot:

```{r, echo = FALSE,fig.width=4.5, fig.height = 4.5, warning = FALSE}
# First we initialize x1 and x2
x1 <- seq(-4, 4, length.out = 1000)
x2 <- seq(-4, 8, length.out = 1000)
# Place points on a grid to plot.
grid <- expand.grid("x1" = x1,
                    "x2" = x2)

# Create a classifier based on the question.
blue <- (1+grid$x1[order(grid$x1)])^2 + (2-grid$x2[order(grid$x1)])^2 > 4
# Order the values for lines function.
x <- grid$x1[order(grid$x1)]

y <- 2 - sqrt(4-(1+grid$x1)^2)
y <- y[order(y)]
y2 <- 2 + sqrt(4-(1+grid$x1)^2)
y2 <- y2[order(y2)]
names <- c("A", "B", "C", "D")
x_id <- c(0, -1, 2, 3)
y_id <- c(0, 1, 2, 8)

plot(grid$x1, grid$x2, col = "blue", 
     xlab = expression(italic(X)[1]), ylab = expression(italic(X)[2]))
# points(grid$x1[!blue], grid$x2[!blue], col = "red")
draw.circle(-1,2,radius = 2, col = "red")
textxy(x_id, y_id, labs = names, cx = 1, dcol = "black")
```



### (d) Argue that while the decision boundary in (c) is not linear in terms of $X_1$ and $X_2$, it is linear in terms of $X_1$, $X_1^2$, $X_2$, and $X_2^2$.

This follows from the expansion of the quadtratic terms in the definition of a circle.


## JWHT. Chp 9 Question 5.

### We have seen that we can fit an SVM with a non-linear kernel in order to perform classification using a non-linear decision boundary. We will now see that we can also obtain a non-linear decision boundary by performing logistic regression using non-linear transformations of the features.

### (a) Generate a data set with $n = 500$ and $p = 2$, such that the observations belong to two classes with a quadratic decision boundary between them. For instance, you can do this as follows:

```{r}
# Set a seed so we can reproduce results.
set.seed(14)

# Draw as book suggests.
x1 <- runif(500) - 0.5
x2 <- runif(500) - 0.5
y <- 1*(x1^2 - x2^2 > 0)
```


### (b) Plot the observations, colored according to their class labels. Your plot should display $X_1$ on the $x$-axis, and $X_2$ on the $y$-axis.

```{r, echo = FALSE, fig.align = "center", fig.asp = .68, fig.width=4.5, warning = FALSE}
plot(x1[y == 1], x2[y == 1], col = "red",
     xlab = expression(italic(X)[1]),
     ylab = expression(italic(X)[2]))
points(x1[y != 1], x2[y != 1], col = "blue")
```

### (c) Fit a logistic regression model to the data, using $X_1$ and $X_2$ as predictors.

We seek:

$$\text{logit}P(Y = 1) = \beta_0 + \beta_1 X_1 + \beta_2 X_2,$$

where $P(Y = 1) = \dfrac{1}{1 + \text{exp}(-[\beta_0 + \beta_1 X_1 + \beta_2 X_2])}$

```{r, echo = FALSE}
# We wrangle the data.
df <- data.frame(cbind(y, x1, x2))

fit_train <- glm(y ~ x1 +x2, family = binomial(link = "logit"),
                 data = df)
```

### (d) Apply this model to the training data in order to obtain a predicted class label for each training observation. Plot the observations, colored according to the predicted class labels. The decision boundary should be linear.

```{r}
pred <- predict(fit_train, type = "response")

auc_roc <- roc(df$y, pred)

best_thresh <- coords(auc_roc, "best", ret= "threshold")


y_pred <- ifelse(pred >= best_thresh, 1, 0)


plot(x1[y_pred == 1], x2[y_pred == 1], col = "red",
     xlab = expression(italic(X)[1]),
     ylab = expression(italic(X)[2]))
points(x1[y_pred != 1], x2[y_pred != 1], col = "blue")
```


### (e) Now fit a logistic regression model to the data using non-linear functions of $X_1$ and $X_2$ as predictors (e.g. $X^2_1$, $X_1 \times X_2$, log($X_2$), and so forth).

```{r, echo = FALSE, fig.align = "center", fig.asp = .68, fig.width=4.5, warning = FALSE}
nonlinear_fit <- glm(y ~ poly(x1, 3) + poly(x2, 3) +
                       x1:x2, data = df)

summary(nonlinear_fit)

select_vars <- step(nonlinear_fit, direction = "backward")

```


### (f) Apply this model to the training data in order to obtain a predicted class label for each training observation. Plot the observations, colored according to the predicted class labels. The decision boundary should be obviously non-linear. If it is not, then repeat (a)-(e) until you come up with an example in which the predicted class labels are obviously non-linear.

```{r, echo = FALSE, fig.align = "center", fig.asp = .68, fig.width=4.5, warning = FALSE}


nonlinear_fit <- glm(y ~ poly(x1, 3) + poly(x2, 3)
                     , data = df)

pred <- predict(nonlinear_fit, type = "response")

auc_roc <- roc(df$y, pred)

best_thresh <- coords(auc_roc, "best", ret= "threshold")


y_pred <- ifelse(pred >= best_thresh, 1, 0)


plot(x1[y_pred == 1], x2[y_pred == 1], col = "red",
     xlab = expression(italic(X)[1]),
     ylab = expression(italic(X)[2]))
points(x1[y_pred != 1], x2[y_pred != 1], col = "blue")
```

### (g) Fit a support vector classifier to the data with $X_1$ and $X_2$ as predictors. Obtain a class prediction for each training observation. Plot the observations, colored according to the predicted class labels.

```{r, echo = FALSE, fig.align = "center", fig.asp = .68, fig.width=4.5, warning = FALSE}
df2 <- df %>%
  mutate(y = ifelse(y == 1, 1, -1)) %>%
  mutate(y = as.factor(y))

set.seed(22)
in_train <- sample(nrow(df2), nrow(df2)*0.8, replace = FALSE)

tune_out <- tune(svm , y~ . , 
                 data = df[in_train , ],
                 kernel = "linear",
                 ranges = list(cost=c(1:100)))

costs <- summary(tune_out)
best_cost <- costs$best.parameters

svm_fit <- svm(y~., data = df2 ,
               kernel = "linear", cost = best_cost,
               scale =FALSE )

plot(svm_fit, df2)
```



### (h) Fit a SVM using a non-linear kernel to the data. Obtain a class prediction for each training observation. Plot the observations, colored according to the predicted class labels.


```{r, echo = FALSE, fig.align = "center", fig.asp = .68, fig.width=4.5, warning = FALSE}

tune_out <- tune(svm , y~ . , 
                 data = df[in_train , ],
                 kernel = "polynomial",
                 degree = 2,
                 ranges = list(cost=c(1:100)))

costs <- summary(tune_out)
best_cost <- costs$best.parameters

svm_fit <- svm(y~., data = df2 ,
               kernel = "polynomial", degree = 2,
               cost = best_cost,
               scale =FALSE )

plot(svm_fit, df2)
```

### (i) Comment on your results.


## JWHT. Chp 9 Question 7.

### In this problem, you will use support vector approaches in order to predict whether a given car gets high or low gas mileage based on the `Auto` data set.

```{r, echo = FALSE}
df <- Auto
```

### (a) Create a binary variable that takes on a 1 for cars with gas mileage above the median, and a 0 for cars with gas mileage below the median.

```{r}
# Create binary as stated above.
df2 <- df %>%
  mutate(above_median = ifelse(mpg > median(df$mpg), 1, -1)) %>%
  mutate(above_median = as.factor(above_median)) %>%
  dplyr::select(-mpg, -name) %>%
  mutate(origin = as.factor(origin))

```

### (b) Fit a support vector classifier to the data with various values of `cost`, in order to predict whether a car gets high or low gas mileage. Report the cross-validation errors associated with different values of this parameter. Comment on your results.

```{r, echo = FALSE}
set.seed(11)
in_train <- sample(nrow(df), nrow(df)*0.8, replace = FALSE)

tune_out <- tune(svm , above_median ~ ., 
                 data = df2[in_train , ],
                 kernel = "linear",
                 ranges =list(cost=c(0.001 , 0.01, 0.1, 1,5,10,100) ))

costs <- summary(tune_out)

table <- costs$performances
kable(table, caption = "Performance for Linear Kernel",
      booktabs = TRUE, format = "latex") %>%
  kable_styling(latex_options=c("hold_position"))


svm_linear <- svm(above_median ~ .,
                  data = df2,
                  kernel = "linear",
                  cost = costs$best.performance,
                  scale =FALSE )

plot(svm_linear, df2, cylinders ~ displacement)
```

### (c) Now repeat (b), this time using SVMs with radial and polynomial basis kernels, with different values of `gamma` and `degree` and `cost`. Comment on your results.

```{r, echo = FALSE}

tune_out <- tune(svm , above_median ~ ., 
                 data = df2[in_train , ],
                 kernel = "polynomial",
                 ranges =list(cost=c(0.001 , 0.01, 0.1, 1,5,10,100),
                              degree = 1:5,
                              gamma = c(0.5,1,2,3,4)))

costs <- summary(tune_out)

table <- costs$performances[48:53,]
kable(table, caption = "Performance for Polynomial Kernel",
      booktabs = TRUE, format = "latex") %>%
  kable_styling(latex_options=c("hold_position"))


tune_out <- tune(svm , above_median ~ ., 
                 data = df2[in_train , ],
                 kernel = "radial",
                 ranges =list(cost=c(0.001 , 0.01, 0.1, 1,5,10,100),
                              gamma = c(0.5,1,2,3,4)))

costs <- summary(tune_out)

table <- costs$performances[8:12,]
kable(table, caption = "Performance for Radial Kernel",
      booktabs = TRUE, format = "latex") %>%
  kable_styling(latex_options=c("hold_position"))

```

### (d) Make some plots to back up your assertions in (b) and (c). Hint: In the lab, we used the `plot()` function for svm objects only in cases with $p = 2$. When $p > 2$, you can use the `plot()` function to create plots displaying pairs of variables at a time. Essentially, instead of typing

```{r, eval=FALSE}
plot(svmfit , dat)
```

### where svmfit contains your fitted model and dat is a data frame containing your data, you can type

```{r, eval=FALSE}
plot(svmfit , dat , x1~x4)
```

### in order to plot just the first and fourth variables. However, you must replace x1 and x4 with the correct variable names. To find out more, type `?plot.svm`.

## JWHT. Chp 9 Question 8.

### This problem involves the `OJ` data set which is part of the `ISLR` package.

### (a) Create a training set containing a random sample of 800 observations, and a test set containing the remaining observations.

### (b) Fit a support vector classifier to the training data using $cost=0.01$, with `Purchase` as the response and the other variables as predictors. Use the `summary()` function to produce summary statistics, and describe the results obtained.

### (c) What are the training and test error rates?

### (d) Use the `tune()` function to select an optimal `cost`. Consider values in the range 0.01 to 10.

### (e) Compute the training and test error rates using this new value for `cost`.

### (f) Repeat parts (b) through (e) using a support vector machine with a radial kernel. Use the default value for `gamma`.

### (g) Repeat parts (b) through (e) using a support vector machine with a polynomial kernel. Set `degree=2`.

### (h) Overall, which approach seems to give the best results on this data?
