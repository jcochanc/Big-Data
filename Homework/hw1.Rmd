---
title: "Homework 1"
author: "Jerson R. Cochancela"
date:  "`r format(Sys.time(), '%B %d, %Y')`"
output: pdf_document
---


```{r, echo = FALSE}
# Libraries.
pacman::p_load(dplyr, kableExtra, knitr, ggplot2, ISLR)
```

Book “An Introduction to Statistical Learning” by James G, Witten D, Hastie T, and Tibshirani (JWHT)

------------

## JWHT. Chapter 3. Question 4. 

### I collect a set of data (n = 100 observations) containing a single predictor and a quantitative response. I then fit a linear regression model to the data, as well as a separate cubic regression, i.e. $Y = \beta_0 + \beta_1 X + \beta_2 X^2 + \beta^3 X^3 + \epsilon$.

### (a) Suppose that the true relationship between X and Y is linear, i.e.$Y = \beta_0 + \beta_1 X + \epsilon$. Consider the training residual sum of squares (RSS) for the linear regression, and also the training RSS for the cubic regression. Would we expect one to be lower than the other, would we expect them to be the same, or is there not enough information to tell? Justify your answer.

We would expect the RSS for training in the cubic setting to be smaller than the training RSS in the linear setting. We can see this to be the case as the noise introduced by $\epsilon$ to be overfit by the cubic regression, leading to smaller RSS. 

### (b) Answer (a) using test rather than training RSS.

Now using the test data, comparing the RSS of the linear vs. the cubic regression, we would expect the linear regression to have a smaller RSS on than the cubic. We can see this as a result of the overfitting done in the cubic setting (knowing that the true relationship is indeed linear). The cubic regression will have trouble fitting the noise whereas, on average, the linear setting will capture the relationship better with smaller RSS.

### (c) Suppose that the true relationship between X and Y is not linear, but we don’t know how far it is from linear. Consider the training RSS for the linear regression, and also the training RSS for the cubic regression. Would we expect one to be lower than the other, would we expect them to be the same, or is there not enough information to tell? Justify your answer.

My intuition is that, in the case where we don't know the true relation \textbf{but it is not linear}, then the cubic RSS will be smaller than the linear RSS. The idea here is that the linear model will not capture the curvature of the data as well as a cubic model may.


### (d) Answer (c) using test rather than training RSS.

Extending the idea above, we would expect that on the held out data, the test data, the RSS for the cubic will continue to be smaller than the linear RSS.

## JWHT. Chapter 3. Question 13 (a)-(g). 

### In this exercise you will create some simulated data and will fit simple linear regression models to it. Make sure to use set.seed(1) prior to starting part (a) to ensure consistent results.

```{r}
# We set the seed as the problem asks.
set.seed(1)
```

### (a) Using the rnorm() function, create a vector, \textbf{x}, containing 100 observations drawn from a $N(0,1)$ distribution. This represents a feature, $X$.

```{r}
# Drawing 100 psuedo RS from N(0, 1).
x <- rnorm(100)
```


### (b) Using the rnorm() function, create a vector, eps, containing 100 observations drawn from a $N(0, 0.25)$ distribution i.e. a normal distribution with mean zero and variance 0.25.

```{r}
# Drawing 100 psuedo RS from N(0, 0.5^2).
eps <- rnorm(100, mean = 0, sd = 0.5)
```

### (c) Using x and eps, generate a vector y according to the model 
$$ Y = -1 + 0.5X + \epsilon. $$
### What is the length of the vector y? What are the values of $\beta_0$ and $\beta_1$ in this linear model?

Before we generate y, we can see that y will have he same dimensions as x and eps. As both are $100 \times 1$ vectors, then their transformation according to the model above will results in a vector $\mathbf{y}_{100 \times 1}$. Additionally, we can see that the intercept is -1, i.e. $\beta_0 = -1$ and the slope is 1/2, i.e. $\beta_1 = 0.5$.

```{r}
# Build y according to the model.
y <- -1 + 0.5*x + eps
```

Below, we confirm the dimensions of y.

```{r}
length(y)
```
### (d) Create a scatterplot displaying the relationship between x andy. Comment on what you observe.

```{r, echo = FALSE, fig.align = "center", fig.asp = .68, fig.width=4.5, warning = FALSE}
# Bind the data for ease in ggplot.
df <- data.frame(cbind(y, x))

# Scatterplot with ggplot.
ggplot(df, aes(x, y)) +
  geom_point() +
  theme_minimal()
```


### (e) Fit a least squares linear model to predict y using x. Comment on the model obtained. How do $\hat{\beta_0}$ and $\hat{\beta_1}$ compare to $\beta_0$ and $\beta_1$?

```{r}
# Now we fit a model to the data.
lm_fit <- lm(y ~ x, data = df)

# Let's examine the estimated intercept and slope.
coefs <- round(summary(lm_fit)$coefficients, 2)

# Clean up p-vals.
coefs[,4] <- "<0.0001"

# Make table.
kable(coefs, caption = "Estimated Coefficients",
      booktabs = TRUE, format = "latex") %>%
  kable_styling(latex_options=c("hold_position"))
```

### (f) Display the least squares line on the scatterplot obtained in (d). Draw the population regression line on the plot, in a different color. Use the legend() command to create an appropriate legend.

```{r, echo = FALSE, fig.align = "center", fig.asp = .68, fig.width=4.5, warning = FALSE}

# Scatterplot with ggplot.
ggplot(df, aes(x, y)) +
  geom_point() +
  geom_abline(intercept = as.numeric(coefs[1, 1]),
              slope = as.numeric(coefs[2,1]), color="red", 
                 linetype="dashed", size=1) 
```

### (g) Now fit a polynomial regression model that predicts y using x and x2. Is there evidence that the quadratic term improves the model fit? Explain your answer.

```{r}
# Fit a quadratic model.
quad_fit <- lm(y ~ poly(x, 2), data = df)

summary(quad_fit)

anova(lm_fit, quad_fit)
```

## JWHT. Chapter 3. Question 14. 

### This problem focuses on the collinearity problem.

### (a) Perform the following commands in R:

```{r}
# Following the book.
set.seed(1)

# Drawing 100 PRV from a U(0, 1).
x1 <- runif(100)

# Creating x2 which follows a model:
# Y = 0.5 x1*U(0,1)/10
x2 <- 0.5 * x1 + rnorm (100) / 10
# Q: Couldn't we have just drawn 100 PRv from U(0, 100)?

# Builing y according to the model below.
y <- 2 + 2 * x1 + 0.3 * x2 + rnorm(100)
```


### The last line corresponds to creating a linear model in which y is a function of x1 and x2. Write out the form of the linear model. What are the regression coefficients?

$$Y = 2 + 2X_1 + 0.3 X_2 + \epsilon$$

where $\beta_0 = 2$, $\beta_1 = 2$ and $\beta_3 = 0.3$. We note that $\epsilon \sim N_(0, 1)$ for 100 draws.

### (b) What is the correlation between x1 and x2? Create a scatterplot displaying the relationship between the variables.

Below we see that the correlation between $X_1$ and $X_2$ is about 0.85.

```{r}
cor(x1, x2)
```

This is no surprise to us as $X_2$ is linearly related, by construction:

$$X_2 = 0.5 X_1 + \epsilon_{x_1}$$
where $\epsilon_{x_1}$ are 100 PSV from a $N(0,1)/10$, wher we have abused the notation but denote the 100 PRV from the standard normal have been divided by 10.


```{r, echo = FALSE, fig.align = "center", fig.asp = .68, fig.width=4.5, warning = FALSE}
# Bind the new data.
df2 <- data.frame(cbind(x1, x2, y))

# Scatterplot with ggplot.
ggplot(df2, aes(x1, x2)) +
  geom_point() 

```

### (c) Using this data, fit a least squares regression to predict y using x1 and x2. Describe the results obtained. What are $\hat{\beta_0}$, $\hat{\beta_1}$, and $\hat{\beta_2}$? 

First we fit the linear model on x1 and x2.

```{r}
# The linear model on x1 and x2.
fit <- lm(y ~ x1 + x2, data = df2)
```

### How do these relate to the true $\beta_0$, $\beta_1$, and $\beta_2$? 

```{r, echo = TRUE}
# Let's examine the estimated intercept and slope.
coefs <- round(summary(fit)$coefficients, 2)

# Clean up p-vals.
coefs[1,4] <- "<0.0001"

# Make table.
kable(coefs, caption = "Estimated Coefficients",
      booktabs = TRUE, format = "latex") %>%
  kable_styling(latex_options=c("hold_position"))
```


```{r, echo = TRUE}
true_betas <- c(2,2,1)
names_betas <- c("Intercept", "x1", "x2")

table <- cbind("Coefficients" = names_betas, "True Beta's" = true_betas)

# Make table.
kable(table, caption = "True Coefficients",
      booktabs = TRUE, format = "latex") %>%
  kable_styling(latex_options=c("hold_position"))

```

### Can you reject the null hypothesis $H_0: \beta_1 = 0$? How about the null hypothesis $H_0: \beta_2 = 0$?

### (d) Now fit a least squares regression to predict y using only x1. Comment on your results. Can you reject the null hypothesis $H_0: \beta_1 = 0$?

```{r}
# Fitting model with only x1.
fit_x1 <- lm(y ~ x1, data = df2)
```

```{r, echo = FALSE}
# Let's examine the estimated intercept and slope.
coefs <- round(summary(fit_x1)$coefficients, 2)

# Clean up p-vals.
coefs[,4] <- "<0.0001"

# Make table.
kable(coefs, caption = "Estimated Coefficients",
      booktabs = TRUE, format = "latex") %>%
  kable_styling(latex_options=c("hold_position"))
```


### (e) Now fit a least squares regression to predict y using only x2. Comment on your results. Can you reject the null hypothesis $H_0: \beta_1 = 0$?


```{r}
# Fit model only on x2.
fit_x2 <- lm(y ~ x2, data = df2)
```

```{r, echo = FALSE}
# Let's examine the estimated intercept and slope.
coefs <- round(summary(fit_x2)$coefficients, 2)

# Clean up p-vals.
coefs[,4] <- "<0.0001"

# Make table.
kable(coefs, caption = "Estimated Coefficients",
      booktabs = TRUE, format = "latex") %>%
  kable_styling(latex_options=c("hold_position"))
```

### (f) Do the results obtained in (c)-(e) contradict each other? Explain your answer.

Yes.

### (g) Now suppose we obtain one additional observation, which was unfortunately mismeasured.

```{r}
x1 <- c(x1 , 0.1)
x2 <- c(x2 , 0.8)
y <- c(y,6)

# Save them for later use.
df3 <- data.frame(cbind(x1, x2, y))
```

## Re-fit the linear models from (c) to (e) using this new data. 

```{r}
# Refitting the three models.
fit <- lm(y ~ x1 + x2, data = df3)

fit_x1 <- lm(y ~ x1, data = df3)

fit_x2 <- lm(y ~ x2, data = df3)
```

### What effect does this new observation have on the each of the models? 

```{r, echo = FALSE}
# Let's examine the estimated intercept and slope.
coefs <- round(summary(fit)$coefficients, 2)
coefs_x1 <- round(summary(fit_x1)$coefficients, 2)
coefs_x2 <- round(summary(fit_x2)$coefficients, 2)

all_coefs <- rbind(coefs, coefs_x1, coefs_x2)

# Clean up p-vals.
all_coefs[-c(2,3),4] <- "<0.0001"

# Make table.
kable(all_coefs, caption = "Estimated Coefficients",
      booktabs = TRUE, format = "latex") %>%
  kable_styling(latex_options=c("hold_position")) %>%
  group_rows("Full Model", 1, 3) %>%
group_rows("X_1 Model", 4, 5) %>%
group_rows("X_2 Model", 6, 7)
```
### In each model, is this observation an outlier? A high-leverage point? Both? Explain your answers.

```{r, echo = FALSE}
par(mfrow=c(1,3))
plot(fit, 5)
plot(fit_x1, 5)
plot(fit_x2, 5)
par(mfrow=c(1,1))

```

## JWHT. Chapter 6. Question 5. 

### It is well-known that ridge regression tends to give similar coefficient values to correlated variables, whereas the lasso may give quite different coefficient values to correlated variables. We will now explore this property in a very simple setting.

### Suppose that $n = 2$, $p = 2$, $x_{11} = x_{12}$, $x_{21} = x_{22}$. Furthermore, suppose that $y_1+y_2 = 0$ and $x_{11}+x_{21} = 0$ and $x_{12}+x_{22} = 0$, so that the estimate for the intercept in a least squares, ridge regression, or lasso model is zero: $\hat{\beta_0} = 0$.

### (a) Write out the ridge regression optimization problem in this setting.

### (b) Argue that in this setting, the ridge coefficient estimates satisfy $\hat{\beta_1} = \hat{\beta_1}$.

### (c) Write out the lasso optimization problem in this setting.

### (d) Argue that in this setting, the lasso coefficients $\hat{\beta_1}$ and $\hat{\beta_2}$ are not unique-in other words, there are many possible solutions to the optimization problem in (c). Describe these solutions.


## JWHT. Chapter 6. Question 9 (a)-(d). 

### In this exercise, we will predict the number of applications received using the other variables in the College data set.

### (a) Split the data set into a training set and a test set.

### (b) Fit a linear model using least squares on the training set, and report the test error obtained.

### (c) Fit a ridge regression model on the training set, with $\lambda$ chosen by cross-validation. Report the test error obtained.

### (d) Fit a lasso model on the training set, with $\lambda$ chosen by crossvalidation. Report the test error obtained, along with the number of non-zero coefficient estimates.


## JWHT. Chapter 4. Question 10 (a)-(d), and (i). For (i), consider using model regularizations for model selection and for now, ignore the last sentence about the experiment with KNN classifier.

### This question should be answered using the Weekly data set, which is part of the ISLR package. This data is similar in nature to the Smarket data from this chapter’s lab, except that it contains 1,089 weekly returns for 21 years, from the beginning of 1990 to the end of 2010.

### (a) Produce some numerical and graphical summaries of the Weekly data. Do there appear to be any patterns?

### (b) Use the full data set to perform a logistic regression with Direction as the response and the five lag variables plus Volume as predictors. Use the summary function to print the results. Do any of the predictors appear to be statistically significant? If so, which ones?

### (c) Compute the confusion matrix and overall fraction of correct predictions. Explain what the confusion matrix is telling you about the types of mistakes made by logistic regression.

### (d) Now fit the logistic regression model using a training data period from 1990 to 2008, with Lag2 as the only predictor. Compute the confusion matrix and the overall fraction of correct predictions for the held out data (that is, the data from 2009 and 2010).

### (i) Experiment with different combinations of predictors, including possible transformations and interactions, for each of the methods. Report the variables, method, and associated confusion matrix that appears to provide the best results on the held out data. Note that you should also experiment with values for K in the KNN classifier.