---
title: "Homework 1"
author: '[Your Name], Collaborator: Jerson R. Cochancela'
date:  "`r format(Sys.time(), '%B %d, %Y')`"
output: pdf_document
---


Book “An Introduction to Statistical Learning” by James G, Witten D, Hastie T, and Tibshirani (JWHT)

------------

## JWHT. Chapter 3. Question 4. 

### I collect a set of data (n = 100 observations) containing a single predictor and a quantitative response. I then fit a linear regression model to the data, as well as a separate cubic regression, i.e. $Y = \beta_0 + \beta_1 X + \beta_2 X^2 + \beta^3 X^3 + \epsilon$.

### (a) Suppose that the true relationship between X and Y is linear, i.e.$Y = \beta_0 + \beta_1 X + \epsilon$. Consider the training residual sum of squares (RSS) for the linear regression, and also the training RSS for the cubic regression. Would we expect one to be lower than the other, would we expect them to be the same, or is there not enough information to tell? Justify your answer.



### (b) Answer (a) using test rather than training RSS.


### (c) Suppose that the true relationship between X and Y is not linear, but we don’t know how far it is from linear. Consider the training RSS for the linear regression, and also the training RSS for the cubic regression. Would we expect one to be lower than the other, would we expect them to be the same, or is there not enough information to tell? Justify your answer.


### (d) Answer (c) using test rather than training RSS.


## JWHT. Chapter 3. Question 13 (a)-(g). 

### In this exercise you will create some simulated data and will fit simple linear regression models to it. Make sure to use set.seed(1) prior to starting part (a) to ensure consistent results.



### (a) Using the rnorm() function, create a vector, \textbf{x}, containing 100 observations drawn from a $N(0,1)$ distribution. This represents a feature, $X$.



### (b) Using the rnorm() function, create a vector, eps, containing 100 observations drawn from a $N(0, 0.25)$ distribution i.e. a normal distribution with mean zero and variance 0.25.


### (c) Using x and eps, generate a vector y according to the model 
$$ Y = -1 + 0.5X + \epsilon. $$
### What is the length of the vector y? What are the values of $\beta_0$ and $\beta_1$ in this linear model?



### (d) Create a scatterplot displaying the relationship between x andy. Comment on what you observe.



### (e) Fit a least squares linear model to predict y using x. Comment on the model obtained. How do $\hat{\beta_0}$ and $\hat{\beta_1}$ compare to $\beta_0$ and $\beta_1$?


### (f) Display the least squares line on the scatterplot obtained in (d). Draw the population regression line on the plot, in a different color. Use the legend() command to create an appropriate legend.



### (g) Now fit a polynomial regression model that predicts y using x and x2. Is there evidence that the quadratic term improves the model fit? Explain your answer.



## JWHT. Chapter 3. Question 14. 

### This problem focuses on the collinearity problem.

### (a) Perform the following commands in R:



### The last line corresponds to creating a linear model in which y is a function of x1 and x2. Write out the form of the linear model. What are the regression coefficients?


### (b) What is the correlation between x1 and x2? Create a scatterplot displaying the relationship between the variables.




### (c) Using this data, fit a least squares regression to predict y using x1 and x2. Describe the results obtained. What are $\hat{\beta_0}$, $\hat{\beta_1}$, and $\hat{\beta_2}$? 



### How do these relate to the true $\beta_0$, $\beta_1$, and $\beta_2$? 



### Can you reject the null hypothesis $H_0: \beta_1 = 0$? How about the null hypothesis $H_0: \beta_2 = 0$?

### (d) Now fit a least squares regression to predict y using only x1. Comment on your results. Can you reject the null hypothesis $H_0: \beta_1 = 0$?



### (e) Now fit a least squares regression to predict y using only x2. Comment on your results. Can you reject the null hypothesis $H_0: \beta_1 = 0$?


### (f) Do the results obtained in (c)-(e) contradict each other? Explain your answer.



### (g) Now suppose we obtain one additional observation, which was unfortunately mismeasured.


### Re-fit the linear models from (c) to (e) using this new data. 


### What effect does this new observation have on the each of the models? 


### In each model, is this observation an outlier? A high-leverage point? Both? Explain your answers.




## JWHT. Chapter 6. Question 5. 

### It is well-known that ridge regression tends to give similar coefficient values to correlated variables, whereas the lasso may give quite different coefficient values to correlated variables. We will now explore this property in a very simple setting.

### Suppose that $n = 2$, $p = 2$, $x_{11} = x_{12}$, $x_{21} = x_{22}$. Furthermore, suppose that $y_1+y_2 = 0$ and $x_{11}+x_{21} = 0$ and $x_{12}+x_{22} = 0$, so that the estimate for the intercept in a least squares, ridge regression, or lasso model is zero: $\hat{\beta_0} = 0$.



### (a) Write out the ridge regression optimization problem in this setting.


### (b) Argue that in this setting, the ridge coefficient estimates satisfy $\hat{\beta_1} = \hat{\beta_2}$.


### (c) Write out the lasso optimization problem in this setting.


### (d) Argue that in this setting, the lasso coefficients $\hat{\beta_1}$ and $\hat{\beta_2}$ are not unique-in other words, there are many possible solutions to the optimization problem in (c). Describe these solutions.




## JWHT. Chapter 6. Question 9 (a)-(d). 

### In this exercise, we will predict the number of applications received using the other variables in the College data set.


### (a) Split the data set into a training set and a test set.



### (b) Fit a linear model using least squares on the training set, and report the test error obtained.


### (c) Fit a ridge regression model on the training set, with $\lambda$ chosen by cross-validation. Report the test error obtained.



### (d) Fit a lasso model on the training set, with $\lambda$ chosen by crossvalidation. Report the test error obtained, along with the number of non-zero coefficient estimates.


## JWHT. Chapter 4. Question 10 (a)-(d), and (i). For (i), consider using model regularizations for model selection and for now, ignore the last sentence about the experiment with KNN classifier.

### This question should be answered using the Weekly data set, which is part of the ISLR package. This data is similar in nature to the Smarket data from this chapter’s lab, except that it contains 1,089 weekly returns for 21 years, from the beginning of 1990 to the end of 2010.


### (a) Produce some numerical and graphical summaries of the Weekly data. Do there appear to be any patterns?



### (b) Use the full data set to perform a logistic regression with Direction as the response and the five lag variables plus Volume as predictors. Use the summary function to print the results. Do any of the predictors appear to be statistically significant? If so, which ones?

### (c) Compute the confusion matrix and overall fraction of correct predictions. Explain what the confusion matrix is telling you about the types of mistakes made by logistic regression.

### (d) Now fit the logistic regression model using a training data period from 1990 to 2008, with Lag2 as the only predictor. Compute the confusion matrix and the overall fraction of correct predictions for the held out data (that is, the data from 2009 and 2010).

### (i) Experiment with different combinations of predictors, including possible transformations and interactions, for each of the methods. Report the variables, method, and associated confusion matrix that appears to provide the best results on the held out data. Note that you should also experiment with values for K in the KNN classifier.
