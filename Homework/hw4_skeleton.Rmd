---
title: "Homework 4"
author: "Jerson R. Cochancela"
date:  "`r format(Sys.time(), '%B %d, %Y')`"
output: pdf_document
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.pos = 'H')
```

```{r, echo = FALSE}
# Libraries.
pacman::p_load(dplyr, kableExtra, knitr,  
               ISLR, caret, tree, rpart,
               rpart.plot, randomForest,
               MASS, gbm)
```


Book “An Introduction to Statistical Learning” by James G, Witten D, Hastie T, and Tibshirani (JWHT)

------------


## Q1. Develop an AdaBoost classifier for the following toy data example by hand. 

```{r, echo = FALSE}
# Read in data.
q1_mat <- readRDS("C:/Users/jcochanc/Documents/Big-Data/Homework/hw4_q1_mat.RDS")

# Rename columns.
colnames(q1_mat) <- c("Y", "X1", "X2")


# Print to screen.
kable(q1_mat, booktabs = TRUE) %>%
  kable_styling(latex_options=c("hold_position"))
```


### Consider only adding a stump tree with a single split on $X_1$ or $X_2$ for each iteration, where the split can only be made at 0, 2, or 4.

### 1) Show the results of weights, “voting” powers, and errors in a table for the first three iterations. 

### 2) Does the algorithm stop at the third iteration? Justify your answer. 
 

## Q2. Apply XGBoost to the following toy example.      

```{r, echo = FALSE}
# Read in data.
q2_mat <- readRDS("C:/Users/jcochanc/Documents/Big-Data/Homework/hw4_q2_mat.RDS")

# Rename columns.
colnames(q2_mat) <- c("Y", "X1", "X2")

# Print to screen.
kable(q2_mat, booktabs = TRUE, format = "latex") %>%
  kable_styling(latex_options=c("hold_position"))
```

### Let us start with $F_0(X) = 1.4$ and use the square loss to add additional regression trees. Suppose that the minimum number of observations in each leaf is 2, and no penalty is placed on the tree structure. 

### 1) Derive the formulae for $g_i$ and $k_i$ (1st and 2nd order derivatives of loss). Grow the first boosted tree. 

### 2) With $F_1(X)$, grow the second tree. 

### 3) If we add a penalty $\gamma$ on the number of leaves. At what value of $\gamma$, will the tree structure in (a) change?   
 

## Q3. JWHT Page  Ch4. Q11 (a-c, g)

### In this problem, you will develop a model to predict whether a given car gets high or low gas mileage based on the Auto data set.

### (a) Create a binary variable, mpg01, that contains a 1 if mpg contains a value above its median, and a 0 if mpg contains a value below its median. You can compute the median using the median() function. Note you may find it helpful to use the data.frame() function to create a single data set containing both mpg01 and the other Auto variables.

### (b) Explore the data graphically in order to investigate the association between mpg01 and the other features. Which of the other features seem most likely to be useful in predicting mpg01? Scatterplots and boxplots may be useful tools to answer this question. Describe your findings.

### (c) Split the data into a training set and a test set.

### (g) Perform KNN on the training data, with several values of K, in order to predict mpg01. Use only the variables that seemed most associated with mpg01 in (b). What test errors do you obtain? Which value of K seems to perform the best on this data set?

## Q4. Apply AdaBoost and XGBoost to JWHT Ch8 Q11. Which method gives the best result? Summarize your findings. 

### This question uses the Caravan data set.

### (a) Create a training set consisting of the first 1,000 observations, and a test set consisting of the remaining observations.

### (b) Fit a boosting model to the training set with Purchase as the response and the other variables as predictors. Use 1,000 trees, and a shrinkage value of 0.01. Which predictors appear to be the most important?

### (c) Use the boosting model to predict the response on the test data. Predict that a person will make a purchase if the estimated probability of purchase is greater than 20 \%. Form a confusion matrix. What fraction of the people predicted to make a purchase do in fact make one? How does this compare with the results obtained from applying KNN or logistic regression to this data set?